{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data-poseidon/sweta/execution-mt/exec-env/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "import sacrebleu\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    output = []\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            output.append(line.strip())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"all_submissions/data_dict.pkl\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Referred from https://github.com/amazon-science/doc-mt-metrics/blob/main/Prism/add_context.py\n",
    "def add_context(orig_txt: List[str], context_same: List[str], \n",
    "                context_other: List[str], sender_ids: List[str], \n",
    "                sep_token: str = \"</s>\", ws: int = 2) -> List[str]:\n",
    "    if not (len(orig_txt) == len(context_same)== len(context_other)):\n",
    "        raise Exception(f'Lengths should match: len(orig_txt)={len(orig_txt)}, len(context_same)={len(context_same)}, len(context_other)={len(context_other)}')\n",
    "    i = 0\n",
    "    augm_txt = []\n",
    "    for i in range(len(orig_txt)):\n",
    "      context_window = []\n",
    "      for j in range(max(0, i - ws), i):\n",
    "        if sender_ids[j] == sender_ids[i]:\n",
    "          context_window.append(context_same[j])\n",
    "        else:\n",
    "          context_window.append(context_other[j])\n",
    "      augm_txt.append(\" {} \".format(sep_token).join(context_window + [orig_txt[i]]))\n",
    "    return augm_txt\n",
    "\n",
    "class DocCometMetric():\n",
    "  def __init__(self, model_name=\"Unbabel/wmt20-comet-qe-da\", batch_size=64, ref_based=True):\n",
    "    checkpoint_path = download_model(model_name)\n",
    "    self.model = load_from_checkpoint(checkpoint_path)\n",
    "    self.batch_size = batch_size\n",
    "    self.model.enable_context()\n",
    "    self.ref_based = ref_based\n",
    "\n",
    "  def get_score(self, source, outputs, references=None):\n",
    "    if not self.ref_based:\n",
    "      del references\n",
    "      return self.model.predict([{\"mt\": y, \"src\": x} for x, y in zip(source, outputs)],\n",
    "        batch_size=self.batch_size, gpus=1, progress_bar=True)['scores']\n",
    "    else:\n",
    "       return self.model.predict([{\"mt\": y, \"ref\":z, \"src\": x} for x, y, z in zip(source, outputs, references)],\n",
    "        batch_size=self.batch_size, gpus=1, progress_bar=False, devices=[self.device_id])['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_metric = load_from_checkpoint(download_model(\"Unbabel/wmt22-comet-da\"))\n",
    "context_metric = DocCometMetric(model_name=\"Unbabel/wmt20-comet-qe-da\", batch_size=256, ref_based=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(df, columns):\n",
    "    score_dict = {}\n",
    "    for col in columns:\n",
    "        try:\n",
    "            score_dict[col] = {}\n",
    "            score_dict[col][\"comet\"] = np.mean(ref_metric.predict([{\"mt\": y, \"ref\":z, \"src\": x} for x, y, z in zip(df[\"source\"].to_list(),\n",
    "                                                                                df[col].to_list(),\n",
    "                                                                                df[\"reference\"].to_list())],\n",
    "                                batch_size=256, gpus=1)['scores'])\n",
    "            score_dict[col][\"chrf\"] = sacrebleu.corpus_chrf(df[col].to_list(), [df[\"reference\"].to_list()]).score\n",
    "            score_dict[col][\"bleu\"] = sacrebleu.corpus_bleu(df[col].to_list(), [df[\"reference\"].to_list()]).score\n",
    "        except:\n",
    "            continue\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"metrics/MuDA\")\n",
    "from muda.langs import create_tagger\n",
    "from muda.metrics import compute_metrics\n",
    "\n",
    "def read_file(fname):\n",
    "    output = []\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            output.append(line.strip())\n",
    "    return output\n",
    "\n",
    "\n",
    "def recursive_map(func: Callable[[Any], Any], obj: Any) -> Any:\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursive_map(func, v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_map(func, v) for v in obj]\n",
    "    else:\n",
    "        return func(obj)\n",
    "\n",
    "from pathlib import Path\n",
    "def get_muda_accuracy_score(\n",
    "    srcs,\n",
    "    refs,\n",
    "    docids,\n",
    "    tgt_lang=\"de\",\n",
    "    awesome_align_model=\"bert-base-multilingual-cased\",\n",
    "    awesome_align_cachedir=None,\n",
    "    load_refs_tags_file=None,\n",
    "    cohesion_threshold=3,\n",
    "    dump_hyps_tags_file=None,\n",
    "    dump_refs_tags_file=None,\n",
    "    dump_stats_file=None,\n",
    "    phenomena=[\"lexical_cohesion\", \"formality\", \"verb_form\", \"pronouns\"],\n",
    "    hyps=None,\n",
    ") -> None:\n",
    "\n",
    "    if not Path(dump_stats_file).is_file():\n",
    "        tagger = create_tagger(\n",
    "            tgt_lang,\n",
    "            align_model=awesome_align_model,\n",
    "            align_cachedir=awesome_align_cachedir,\n",
    "            cohesion_threshold=cohesion_threshold,\n",
    "        )\n",
    "\n",
    "        if not load_refs_tags_file:\n",
    "            preproc = tagger.preprocess(srcs, refs, docids)\n",
    "            tagged_refs = []\n",
    "            for doc in zip(*preproc):\n",
    "                tagged_doc = tagger.tag(*doc, phenomena=phenomena)\n",
    "                tagged_refs.append(tagged_doc)\n",
    "        else:\n",
    "            tagged_refs = json.load(open(load_refs_tags_file))\n",
    "\n",
    "        preproc = tagger.preprocess(srcs, hyps, docids)\n",
    "        tagged_hyps = []\n",
    "        for doc in zip(*preproc):\n",
    "            tagged_doc = tagger.tag(*doc, phenomena=phenomena)\n",
    "            tagged_hyps.append(tagged_doc)\n",
    "\n",
    "        tag_prec, tag_rec, tag_f1 = compute_metrics(tagged_refs, tagged_hyps)\n",
    "        stat_dicts = []\n",
    "        for tag in tag_f1:\n",
    "            print(\n",
    "                f\"{tag} -- Prec: {tag_prec[tag]:.2f} Rec: {tag_rec[tag]:.2f} F1: {tag_f1[tag]:.2f}\"\n",
    "            )\n",
    "            stat_dicts.append(\n",
    "                {\n",
    "                    \"tag\": tag,\n",
    "                    \"precision\": tag_prec[tag],\n",
    "                    \"recall\": tag_rec[tag],\n",
    "                    \"f1\": tag_f1[tag],\n",
    "                }\n",
    "            )\n",
    "        with open(dump_stats_file, \"w\") as f:\n",
    "            for d in stat_dicts:\n",
    "                f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        if dump_hyps_tags_file:\n",
    "            with open(dump_hyps_tags_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(recursive_map(lambda t: t._asdict(), tagged_refs), f, indent=2)\n",
    "\n",
    "        if not load_refs_tags_file and dump_refs_tags_file:\n",
    "            with open(dump_refs_tags_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(recursive_map(lambda t: t._asdict(), tagged_refs), f, indent=2)\n",
    "    else:\n",
    "        print(f\"{dump_stats_file} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict_all_lps = {}\n",
    "for lp in ['en_nl', 'en_pt', 'en_de', 'en_ko', 'en_fr']:\n",
    "    submission_cols = list(data_dict[lp.replace(\"-\", '_')].keys())\n",
    "    test_df = pd.read_csv(f\"all_submissions/{lp.replace('_', '-')}.csv\")\n",
    "    test_df.fillna('', inplace=True)\n",
    "    # score_dict_all_lps[lp] = get_scores(test_df, submission_cols)\n",
    "\n",
    "    # MuDA\n",
    "    df = test_df[test_df.source_language == \"en\"]\n",
    "    src_lang, tgt_lang = lp.split(\"_\")\n",
    "\n",
    "    for col in submission_cols:\n",
    "        test_df[f\"{col}-comet\"] = ref_metric.predict([{\"mt\": y, \"ref\":z, \"src\": x} for x, y, z in zip(test_df[\"source\"].to_list(),\n",
    "                                                                                test_df[col].to_list(),\n",
    "                                                                                test_df[\"reference\"].to_list())],\n",
    "                                batch_size=256, gpus=1)['scores']\n",
    "        get_muda_accuracy_score(\n",
    "            df[\"source\"].to_list(),\n",
    "            df[\"reference\"].to_list(),\n",
    "            df[\"doc_id\"].to_list(),\n",
    "            hyps=df[col].to_list(),\n",
    "            tgt_lang=tgt_lang,\n",
    "            awesome_align_model=\"bert-base-multilingual-cased\",\n",
    "            awesome_align_cachedir=None,\n",
    "            dump_hyps_tags_file=f\"muda_accuracy_results/{tgt_lang}.{col}.tags.json\",\n",
    "            dump_refs_tags_file=f\"muda_accuracy_results/{tgt_lang}.ref.tags.json\",\n",
    "            dump_stats_file=f\"muda_accuracy_results/{tgt_lang}.{col}.stats.json\",\n",
    "            phenomena=[\"lexical_cohesion\", \"formality\", \"verb_form\", \"pronouns\"],\n",
    "            cohesion_threshold=3,\n",
    "        )\n",
    "\n",
    "    # Context Comet QE\n",
    "    for col in submission_cols:\n",
    "        doc_dfs = []\n",
    "        for _, df_group in test_df.groupby([\"doc_id\"]):\n",
    "            df_group['seg_id'] = list(range(len(df_group)))\n",
    "            df_group[f\"source_with_context\"]  = add_context(\n",
    "                                                    orig_txt=df_group[\"source\"].to_list(),\n",
    "                                                    context_same=df_group[\"source\"].to_list(),\n",
    "                                                    context_other=df_group[col].to_list(),\n",
    "                                                    sender_ids=df_group[\"sender\"].to_list(),\n",
    "                                                    sep_token=context_metric.model.encoder.tokenizer.sep_token,)\n",
    "            df_group[f\"mt_with_context\"]  = add_context(\n",
    "                                                    orig_txt=df_group[col].to_list(),\n",
    "                                                    context_same=df_group[col].to_list(),\n",
    "                                                    context_other=df_group[\"source\"].to_list(),\n",
    "                                                    sender_ids=df_group[\"sender\"].to_list(),\n",
    "                                                    sep_token=context_metric.model.encoder.tokenizer.sep_token,)\n",
    "            doc_dfs.append(df_group)\n",
    "\n",
    "        dfs_all = pd.concat(doc_dfs)\n",
    "        test_df[f\"{col}-context-comet-qe\"] = context_metric.get_score(dfs_all[f\"source_with_context\"], dfs_all[f\"mt_with_context\"])\n",
    "        score_dict_all_lps[lp][col][\"context-comet-qe\"] =  np.mean(test_df[f\"{col}-context-comet-qe\"])\n",
    "    test_df.to_csv(f\"all_submissions/{lp.replace('_', '-')}-scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_cols = ['submission_DeepText_Lab',\n",
    "                   'submission_HW-TSC',\n",
    "                   'submission_MULTITAN-GML',\n",
    "                   'submission_ADAPT',\n",
    "                   'submission_SheffieldGATE',\n",
    "                   'submission_clteam',\n",
    "                   'submission_DCUGenNLP',\n",
    "                   'submission_unbabel+it',\n",
    "                   'submission_baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict_all_lps_bydir = {}\n",
    "for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "    test_df = pd.read_csv(f\"all_submissions/{lp.replace('_', '-')}-scores.csv\")\n",
    "    score_dict_all_lps_bydir[lp] = {}\n",
    "    for dir, dir_df in test_df.groupby(\"source_language\"):\n",
    "        score_dict_all_lps_bydir[lp][dir] = {}\n",
    "        for col in submission_cols:\n",
    "            if f\"{col}-comet\" in dir_df.columns:\n",
    "                score_dict_all_lps_bydir[lp][dir][col] = dir_df[f\"{col}-comet\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in submission_cols:\n",
    "    scores_str = f\"{col}\\t\"\n",
    "    for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "        src_lang, tgt_lang = lp.split(\"_\")\n",
    "        if col in score_dict_all_lps_bydir[lp][src_lang]:\n",
    "            scores_str+=f\" & {score_dict_all_lps_bydir[lp][src_lang][col]*100:.2f} & {score_dict_all_lps_bydir[lp][tgt_lang.replace('pt', 'pt-br')][col]*100:.2f} \"\n",
    "        else:\n",
    "            scores_str+=f\" & & \"\n",
    "    print(col, scores_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict_all_lps_bydir = {}\n",
    "for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "    test_df = pd.read_csv(f\"all_submissions/{lp.replace('_', '-')}-scores.csv\")\n",
    "    score_dict_all_lps_bydir[lp] = {}\n",
    "    for dir, dir_df in test_df.groupby(\"source_language\"):\n",
    "        score_dict_all_lps_bydir[lp][dir] = {}\n",
    "        for col in submission_cols:\n",
    "            if f\"{col}-context-comet-qe\" in dir_df.columns:\n",
    "                score_dict_all_lps_bydir[lp][dir][col] = dir_df[f\"{col}-context-comet-qe\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in submission_cols:\n",
    "    scores_str = f\"{col}\\t\"\n",
    "    for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "        src_lang, tgt_lang = lp.split(\"_\")\n",
    "        if col in score_dict_all_lps_bydir[lp][src_lang]:\n",
    "            scores_str+=f\" & {score_dict_all_lps_bydir[lp][src_lang][col]*100:.2f} & {score_dict_all_lps_bydir[lp][tgt_lang.replace('pt', 'pt-br')][col]*100:.2f} \"\n",
    "        else:\n",
    "            scores_str+=f\" & & \"\n",
    "    print(scores_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "score_dict_all_lps_bydir = {}\n",
    "for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "    test_df = pd.read_csv(f\"all_submissions/{lp.replace('_', '-')}-scores.csv\")\n",
    "    test_df.fillna('', inplace=True)\n",
    "    score_dict_all_lps_bydir[lp] = {}\n",
    "    for dir, dir_df in test_df.groupby(\"source_language\"):\n",
    "        score_dict_all_lps_bydir[lp][dir] = {}\n",
    "        for col in submission_cols:\n",
    "            if f\"{col}\" in dir_df.columns:\n",
    "                score_dict_all_lps_bydir[lp][dir][col] =  sacrebleu.corpus_chrf(dir_df[col].to_list(), [dir_df[\"reference\"].to_list()]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in submission_cols:\n",
    "    scores_str = f\"{col}\\t\"\n",
    "    for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "        src_lang, tgt_lang = lp.split(\"_\")\n",
    "        if col in score_dict_all_lps_bydir[lp][src_lang]:\n",
    "            scores_str+=f\" & {score_dict_all_lps_bydir[lp][src_lang][col]:.2f} & {score_dict_all_lps_bydir[lp][tgt_lang.replace('pt', 'pt-br')][col]:.2f} \"\n",
    "        else:\n",
    "            scores_str+=f\" & & \"\n",
    "    print(scores_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "score_dict_all_lps_bydir = {}\n",
    "for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "    test_df = pd.read_csv(f\"all_submissions/{lp.replace('_', '-')}-scores.csv\")\n",
    "    test_df.fillna('', inplace=True)\n",
    "    score_dict_all_lps_bydir[lp] = {}\n",
    "    for dir, dir_df in test_df.groupby(\"source_language\"):\n",
    "        score_dict_all_lps_bydir[lp][dir] = {}\n",
    "        for col in submission_cols:\n",
    "            if f\"{col}\" in dir_df.columns:\n",
    "                score_dict_all_lps_bydir[lp][dir][col] =  sacrebleu.corpus_bleu(dir_df[col].to_list(), [dir_df[\"reference\"].to_list()]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in submission_cols:\n",
    "    scores_str = f\"{col}\\t\"\n",
    "    for lp in ['en_de', 'en_fr', 'en_nl', 'en_pt', 'en_ko']:\n",
    "        src_lang, tgt_lang = lp.split(\"_\")\n",
    "        if col in score_dict_all_lps_bydir[lp][src_lang]:\n",
    "            scores_str+=f\" & {score_dict_all_lps_bydir[lp][src_lang][col]:.2f} & {score_dict_all_lps_bydir[lp][tgt_lang.replace('pt', 'pt-br')][col]:.2f} \"\n",
    "        else:\n",
    "            scores_str+=f\" & & \"\n",
    "    print(scores_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW-TSC vs Unbabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "score_dict_all_lps_bydir = {}\n",
    "lang_threshold = {\"de\": 31,\"fr\": 33, \"nl\": 35, \"ko\": 48, \"pt\": 28}\n",
    "\n",
    "test_df = pd.read_csv(f\"all_submissions/en-de-scores.csv\")\n",
    "lang_threshold = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df.doc_id.unique()), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "scores_ours = []\n",
    "scores_hw_tsc = []\n",
    "for gr, gr_df in test_df.groupby(\"doc_id\"):\n",
    "    if len(gr_df) >= lang_threshold: continue\n",
    "    scores_ours.extend(gr_df['submission_unbabel+it-comet'])\n",
    "    scores_hw_tsc.extend(gr_df['submission_HW-TSC-comet'])\n",
    "\n",
    "print(np.mean(scores_ours), np.mean(scores_hw_tsc), len(scores_ours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "scores_ours = []\n",
    "scores_hw_tsc = []\n",
    "for gr, gr_df in test_df.groupby(\"doc_id\"):\n",
    "    if len(gr_df) < lang_threshold: continue\n",
    "    scores_ours.extend(gr_df['submission_unbabel+it-comet'])\n",
    "    scores_hw_tsc.extend(gr_df['submission_HW-TSC-comet'])\n",
    "\n",
    "print(np.mean(scores_ours), np.mean(scores_hw_tsc), len(scores_ours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muda results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"all_submissions/data_dict.pkl\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['en_nl', 'en_pt', 'en_de', 'en_ko', 'en_fr'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "langs = [\"de\", \"fr\", \"pt\", \"nl\", \"ko\"]\n",
    "\n",
    "def load_jsonl_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        json_list = list(f)\n",
    "\n",
    "    return [json.loads(l) for l in json_list]\n",
    "\n",
    "df_dict = {\n",
    "    \"model\": [],\n",
    "    \"lp\": [],\n",
    "    \"tag\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1\": [],\n",
    "}\n",
    "\n",
    "data = []\n",
    "for tgt_lang in langs:\n",
    "    for col in data_dict[f\"en_{tgt_lang}\"]:\n",
    "        try:\n",
    "            stats = load_jsonl_file(\n",
    "                f\"muda_accuracy_results/{tgt_lang}.{col}.stats.json\"\n",
    "            )\n",
    "            for s in stats:\n",
    "                for metric in [\"precision\", \"recall\", \"f1\"]:\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"model\": col.split(\"submission_\")[1].replace('DeepText_Lab','DeepText Lab').replace('clteam','CLTeam').replace('unbabel+it','Unbabel-IT').replace('baseline','NLLB-3.3B'),\n",
    "                            \"lp\": tgt_lang,\n",
    "                            \"tag\": s[\"tag\"].replace('verb_form', 'Verb Form').replace('lexical_cohesion', 'Lexical Cohesion').replace('pronouns', 'Pronouns').replace('formality', 'Formality'),\n",
    "                            \"metric\": metric,\n",
    "                            \"value\": s[metric],\n",
    "                        }\n",
    "                    )\n",
    "        except:\n",
    "            continue\n",
    "df = pd.DataFrame().from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "# activate tex\n",
    "plt.rc(\"text\", usetex=False)\n",
    "sns.set_context(\"paper\", font_scale=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages  # Import PdfPages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_models = df['model'].unique().tolist()\n",
    "other_models.remove('NLLB-3.3B')\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\", len(other_models))\n",
    "\n",
    "model_palette = {model: color for model, color in zip(other_models, palette)}\n",
    "model_palette['NLLB-3.3B'] = 'grey'  # Set 'nllb-3.3b' to grey\n",
    "model_order = ['NLLB-3.3B'] + other_models\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig, axs = plt.subplots(len(df['lp'].unique()), 1, figsize=(12, 8 * len(df['lp'].unique())), sharex=True)\n",
    "    for i, lp in  enumerate(df['lp'].unique()):\n",
    "        # Filter the dataframe for the current lp\n",
    "        df_lp = df[df['lp'] == lp]\n",
    "        \n",
    "        # Create a barplot for the current lp\n",
    "        sns.barplot(data=df_lp, \n",
    "                    x=\"tag\", \n",
    "                    y=\"value\", \n",
    "                    hue=\"model\", \n",
    "                    palette=model_palette, \n",
    "                    hue_order=model_order, \n",
    "                    ax=axs[i], \n",
    "                    legend=(i==0))\n",
    "\n",
    "        # Add vertical lines between x-axis categories\n",
    "        for j in range(len(df[df['lp'] == \"fr\"]['tag'].unique()) - 1):\n",
    "            axs[i].axvline(x=j + 0.5, color='black', linestyle='--', linewidth=0.8)\n",
    "        \n",
    "        \n",
    "        # Set the plot title and labels\n",
    "        axs[i].set_title(f'EN-{lp.upper()}')\n",
    "        axs[i].set_ylim(0, 1)  # Set y-axis limits between 0 and 1\n",
    "        axs[i].set_ylabel('F1 Score')\n",
    "        axs[i].set_xlabel('')\n",
    "        if i == 0:\n",
    "            handles, labels = axs[0].get_legend_handles_labels()  # Get handles and labels\n",
    "            axs[i].legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.4), ncol=3)  # Place legend above the plot\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "with PdfPages('plots/all_barplots.pdf') as pdf:\n",
    "    pdf.savefig(fig, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context MQM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ende_score = pd.read_csv(\"all_submissions/en-de-scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dfs = []\n",
    "for _, df_group in ende_score.groupby([\"doc_id\"]):\n",
    "    df_group['segment_id'] = list(range(len(df_group)))\n",
    "    doc_dfs.append(df_group)\n",
    "dfs_all = pd.concat(doc_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(f\"human_eval/docsqm2deu_ann.csv\")\n",
    "scores_df.drop_duplicates(subset=['model_app'], keep='last', inplace=True) \n",
    "scores_df.rename(columns={\"sent_id\": \"segment_id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_HW-TSC & 89.12 & 100 & 88 & 59 & -0.554\n",
      "submission_ADAPT & 82.61 & 158 & 139 & 99 & -0.903\n",
      "submission_SheffieldGATE & 77.95 & 220 & 178 & 95 & -1.009\n",
      "submission_clteam & 86.28 & 139 & 82 & 79 & -0.656\n",
      "submission_DCUGenNLP & 83.10 & 143 & 158 & 80 & -0.849\n",
      "submission_unbabel+it & 94.41 & 51 & 47 & 18 & -0.228\n",
      "submission_baseline & 80.50 & 161 & 143 & 117 & -1.002\n"
     ]
    }
   ],
   "source": [
    "for system in ['submission_HW-TSC',  'submission_ADAPT', 'submission_SheffieldGATE', 'submission_clteam',  'submission_DCUGenNLP', 'submission_unbabel+it', 'submission_baseline']:\n",
    "    gpt_out = pd.read_csv(f\"gpt-4o-mini/gpt-4o-mini-en-de-{system}.csv\")\n",
    "    merged_df = gpt_out.merge(dfs_all, on=['doc_id', 'segment_id'])\n",
    "    error_counts = (' & ').join(list(map(str, gpt_out[['gpt-4o-mini-minor', 'gpt-4o-mini-major',  'gpt-4o-mini-critical-count']].sum().values)))\n",
    "    print(f\"{system} & {len(gpt_out[gpt_out['gpt-4o-mini-score']==0])/len(gpt_out)*100:.2f} & {error_counts} & {gpt_out['gpt-4o-mini-score'].mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exec-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
