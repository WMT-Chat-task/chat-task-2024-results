{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "import sacrebleu\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    output = []\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            output.append(line.strip())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_submissions/data_dict.pkl\", \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Referred from https://github.com/amazon-science/doc-mt-metrics/blob/main/Prism/add_context.py\n",
    "def add_context(orig_txt: List[str], context_same: List[str], \n",
    "                context_other: List[str], sender_ids: List[str], \n",
    "                sep_token: str = \"</s>\", ws: int = 2) -> List[str]:\n",
    "    if not (len(orig_txt) == len(context_same)== len(context_other)):\n",
    "        raise Exception(f'Lengths should match: len(orig_txt)={len(orig_txt)}, len(context_same)={len(context_same)}, len(context_other)={len(context_other)}')\n",
    "    i = 0\n",
    "    augm_txt = []\n",
    "    for i in range(len(orig_txt)):\n",
    "      context_window = []\n",
    "      for j in range(max(0, i - ws), i):\n",
    "        if sender_ids[j] == sender_ids[i]:\n",
    "          context_window.append(context_same[j])\n",
    "        else:\n",
    "          context_window.append(context_other[j])\n",
    "      augm_txt.append(\" {} \".format(sep_token).join(context_window + [orig_txt[i]]))\n",
    "    return augm_txt\n",
    "\n",
    "class DocCometMetric():\n",
    "  def __init__(self, model_name=\"Unbabel/wmt20-comet-qe-da\", batch_size=64, ref_based=True):\n",
    "    checkpoint_path = download_model(model_name)\n",
    "    self.model = load_from_checkpoint(checkpoint_path)\n",
    "    self.batch_size = batch_size\n",
    "    self.model.enable_context()\n",
    "    self.ref_based = ref_based\n",
    "\n",
    "  def get_score(self, source, outputs, references=None):\n",
    "    if not self.ref_based:\n",
    "      del references\n",
    "      return self.model.predict([{\"mt\": y, \"src\": x} for x, y in zip(source, outputs)],\n",
    "        batch_size=self.batch_size, gpus=1, progress_bar=True)['scores']\n",
    "    else:\n",
    "       return self.model.predict([{\"mt\": y, \"ref\":z, \"src\": x} for x, y, z in zip(source, outputs, references)],\n",
    "        batch_size=self.batch_size, gpus=1, progress_bar=False, devices=[self.device_id])['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_metric = load_from_checkpoint(download_model(\"Unbabel/wmt22-comet-da\"))\n",
    "context_metric = DocCometMetric(model_name=\"Unbabel/wmt20-comet-qe-da\", batch_size=256, ref_based=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(df, columns):\n",
    "    score_dict = {}\n",
    "    for col in columns:\n",
    "        try:\n",
    "            score_dict[col] = {}\n",
    "            score_dict[col][\"comet\"] = np.mean(ref_metric.predict([{\"mt\": y, \"ref\":z, \"src\": x} for x, y, z in zip(df[\"source\"].to_list(),\n",
    "                                                                                df[col].to_list(),\n",
    "                                                                                df[\"reference\"].to_list())],\n",
    "                                batch_size=256, gpus=1)['scores'])\n",
    "            score_dict[col][\"chrf\"] = sacrebleu.corpus_chrf(df[col].to_list(), [df[\"reference\"].to_list()]).score\n",
    "            score_dict[col][\"bleu\"] = sacrebleu.corpus_bleu(df[col].to_list(), [df[\"reference\"].to_list()]).score\n",
    "        except:\n",
    "            continue\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from typing import Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/mnt/data-poseidon/sweta/chat-translation-generation/wmt24-chat-translation/metrics/MuDA\")\n",
    "from muda.langs import create_tagger\n",
    "from muda.metrics import compute_metrics\n",
    "\n",
    "def read_file(fname):\n",
    "    output = []\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            output.append(line.strip())\n",
    "    return output\n",
    "\n",
    "\n",
    "def recursive_map(func: Callable[[Any], Any], obj: Any) -> Any:\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursive_map(func, v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_map(func, v) for v in obj]\n",
    "    else:\n",
    "        return func(obj)\n",
    "\n",
    "\n",
    "def get_muda_accuracy_score(\n",
    "    srcs,\n",
    "    refs,\n",
    "    docids,\n",
    "    tgt_lang=\"de\",\n",
    "    awesome_align_model=\"bert-base-multilingual-cased\",\n",
    "    awesome_align_cachedir=None,\n",
    "    load_refs_tags_file=None,\n",
    "    cohesion_threshold=3,\n",
    "    dump_hyps_tags_file=None,\n",
    "    dump_refs_tags_file=None,\n",
    "    dump_stats_file=None,\n",
    "    phenomena=[\"lexical_cohesion\", \"formality\", \"verb_form\", \"pronouns\"],\n",
    "    hyps=None,\n",
    ") -> None:\n",
    "\n",
    "    tagger = create_tagger(\n",
    "        tgt_lang,\n",
    "        align_model=awesome_align_model,\n",
    "        align_cachedir=awesome_align_cachedir,\n",
    "        cohesion_threshold=cohesion_threshold,\n",
    "    )\n",
    "\n",
    "    if not load_refs_tags_file:\n",
    "        preproc = tagger.preprocess(srcs, refs, docids)\n",
    "        tagged_refs = []\n",
    "        for doc in zip(*preproc):\n",
    "            tagged_doc = tagger.tag(*doc, phenomena=phenomena)\n",
    "            tagged_refs.append(tagged_doc)\n",
    "    else:\n",
    "        tagged_refs = json.load(open(load_refs_tags_file))\n",
    "\n",
    "    preproc = tagger.preprocess(srcs, hyps, docids)\n",
    "    tagged_hyps = []\n",
    "    for doc in zip(*preproc):\n",
    "        tagged_doc = tagger.tag(*doc, phenomena=phenomena)\n",
    "        tagged_hyps.append(tagged_doc)\n",
    "\n",
    "    tag_prec, tag_rec, tag_f1 = compute_metrics(tagged_refs, tagged_hyps)\n",
    "    stat_dicts = []\n",
    "    for tag in tag_f1:\n",
    "        print(\n",
    "            f\"{tag} -- Prec: {tag_prec[tag]:.2f} Rec: {tag_rec[tag]:.2f} F1: {tag_f1[tag]:.2f}\"\n",
    "        )\n",
    "        stat_dicts.append(\n",
    "            {\n",
    "                \"tag\": tag,\n",
    "                \"precision\": tag_prec[tag],\n",
    "                \"recall\": tag_rec[tag],\n",
    "                \"f1\": tag_f1[tag],\n",
    "            }\n",
    "        )\n",
    "    with open(dump_stats_file, \"w\") as f:\n",
    "        for d in stat_dicts:\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if dump_hyps_tags_file:\n",
    "        with open(dump_hyps_tags_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(recursive_map(lambda t: t._asdict(), tagged_refs), f, indent=2)\n",
    "\n",
    "    if not load_refs_tags_file and dump_refs_tags_file:\n",
    "        with open(dump_refs_tags_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(recursive_map(lambda t: t._asdict(), tagged_refs), f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict_all_lps = {}\n",
    "for lp in ['en_nl', 'en_pt', 'en_de', 'en_ko', 'en_fr']:\n",
    "    submission_cols = list(data_dict[lp.replace(\"-\", '_')].keys())\n",
    "    test_df = pd.read_csv(f\"all_submissions/{lp.replace('_', '-')}.csv\")\n",
    "    test_df.fillna('', inplace=True)\n",
    "    score_dict_all_lps[lp] = get_scores(test_df, submission_cols)\n",
    "\n",
    "    # MuDA\n",
    "    df = test_df[test_df.source_language == \"en\"]\n",
    "    src_lang, tgt_lang = lp.split(\"_\")\n",
    "    for col in submission_cols:\n",
    "        get_muda_accuracy_score(\n",
    "            df[\"source\"].to_list(),\n",
    "            df[\"reference\"].to_list(),\n",
    "            df[\"doc_id\"].to_list(),\n",
    "            hyps=df[col].to_list(),\n",
    "            tgt_lang=tgt_lang,\n",
    "            awesome_align_model=\"bert-base-multilingual-cased\",\n",
    "            awesome_align_cachedir=None,\n",
    "            dump_hyps_tags_file=f\"muda_accuracy_results/{tgt_lang}.{col}.tags.json\",\n",
    "            dump_refs_tags_file=f\"muda_accuracy_results/{tgt_lang}.ref.tags.json\",\n",
    "            dump_stats_file=f\"muda_accuracy_results/{tgt_lang}.{col}.stats.json\",\n",
    "            phenomena=[\"lexical_cohesion\", \"formality\", \"verb_form\", \"pronouns\"],\n",
    "            cohesion_threshold=3,\n",
    "        )\n",
    "\n",
    "    # Context Comet QE\n",
    "    for col in submission_cols:\n",
    "        doc_dfs = []\n",
    "        for _, df_group in test_df.groupby([\"doc_id\"]):\n",
    "            df_group['seg_id'] = list(range(len(df_group)))\n",
    "            df_group[f\"source_with_context\"]  = add_context(\n",
    "                                                    orig_txt=df_group[\"source\"].to_list(),\n",
    "                                                    context_same=df_group[\"source\"].to_list(),\n",
    "                                                    context_other=df_group[col].to_list(),\n",
    "                                                    sender_ids=df_group[\"sender\"].to_list(),\n",
    "                                                    sep_token=context_metric.model.encoder.tokenizer.sep_token,)\n",
    "            df_group[f\"mt_with_context\"]  = add_context(\n",
    "                                                    orig_txt=df_group[col].to_list(),\n",
    "                                                    context_same=df_group[col].to_list(),\n",
    "                                                    context_other=df_group[\"source\"].to_list(),\n",
    "                                                    sender_ids=df_group[\"sender\"].to_list(),\n",
    "                                                    sep_token=context_metric.model.encoder.tokenizer.sep_token,)\n",
    "            doc_dfs.append(df_group)\n",
    "\n",
    "        dfs_all = pd.concat(doc_dfs)\n",
    "        score_dict_all_lps[lp][col][\"context-comet-qe\"] =  np.mean(context_metric.get_score(dfs_all[f\"source_with_context\"], dfs_all[f\"mt_with_context\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict_all_lps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
